# Project Objectives
- Optimize LLM inference pipelines for GPU-based systems
- Examine improvements in latency, throughput, and resource utilization
- Experiment with techniques in GPU optimization and scalable deployment

# NVIDIA Nsight Compute 
#### Part 1:
1. Step 1:
   ![First Image](https://github.com/ramanahm1/LLM-Inferencing/blob/main/screenshots/cmd_1.png)

2. Step 2:
   ![Second Image](https://github.com/ramanahm1/LLM-Inferencing/blob/main/screenshots/nsight_kernel_1.png)



3. Step 3:
    ![Third Image](https://github.com/ramanahm1/LLM-Inferencing/blob/main/screenshots/nsight_kernel_2.png)

4. Step 4:

    ![Fourth Image](https://github.com/ramanahm1/LLM-Inferencing/blob/main/screenshots/cmd_2.png)


# [WIP]
- Uploading performance optimization scenarios 
